{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соревнование: San Francisco Crime Classification\n",
    "\n",
    "_Given time and location, you must predict the category of crime that occurred. Kaggle is hosting this competition for the machine learning community to use for fun and practice._\n",
    "\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка работы. Загрузка библиотек и настройка отображения\n",
    "Импорты и настроийки, которые необходимы для шаблона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка, очистка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка тренировочных данных из csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем преступления с аномальными координатами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data[(train_data['X']!=-120.5) | (train_data['Y']!=90.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного преобразуем данные, конвертируем строку дата в DateTime объект, а другие столбцы конвертируем в категориальные переменные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.Dates = pd.to_datetime(train_data.Dates)\n",
    "train_data.Category = train_data.Category.astype('category')\n",
    "train_data.Descript = train_data.Descript.astype('category')\n",
    "train_data.DayOfWeek = train_data.DayOfWeek.astype('category')\n",
    "train_data.PdDistrict = train_data.PdDistrict.astype('category')\n",
    "train_data.Resolution = train_data.Resolution.astype('category')\n",
    "train_data.Address = train_data.Address.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем нашу дату на час, день месяца, месяц и год, чтобы получить больше информации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_time(data):\n",
    "    data['Hour'] = data.Dates.apply(lambda x: x.hour)\n",
    "    data['Day'] = data.Dates.apply(lambda x: x.day)\n",
    "    data['Month'] = data.Dates.apply(lambda x: x.month)\n",
    "    data['Year'] = data.Dates.apply(lambda x: x.year)\n",
    "    return data\n",
    "train_data = convert_time(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Построение дополнительных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем дополнительные функции, чтобы преобразовать наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiature_converters = dict()\n",
    "\n",
    "def feature_binarizer(data, feature_name, fit=False):\n",
    "    if(fit):\n",
    "        fiature_converters[feature_name]=preprocessing.LabelBinarizer()\n",
    "        #fiature_converters[feature_name]=preprocessing.OneHotEncoder()\n",
    "        fiature_converters[feature_name].fit(data[feature_name].values)\n",
    "    return fiature_converters[feature_name].transform(data[feature_name].values)\n",
    "\n",
    "def conv_street(street):\n",
    "    if street.find('/') != -1:\n",
    "        return map(str.strip, street.split('/'))    \n",
    "    pos = street.find('Block of ')\n",
    "    if pos != -1:        \n",
    "        return [street[pos+9:]]    \n",
    "    return [street]\n",
    "\n",
    "def build_streets(data):\n",
    "    streets = set()\n",
    "    for x in data.Address[0:10]:\n",
    "        streets |= set(conv_street(x))\n",
    "    return streets\n",
    "\n",
    "def append_streets(adr_column, streets):\n",
    "    streets_cols = np.zeros((len(adr_column),len(streets)), dtype=int)\n",
    "    for i, street in enumerate(streets):\n",
    "        for j, address in enumerate(adr_column):\n",
    "            if address.find(street) != -1:\n",
    "                streets_cols[j,i] = 1\n",
    "    return streets_cols\n",
    "\n",
    "streets = build_streets(train_data)\n",
    "\n",
    "coords_scaler = preprocessing.MinMaxScaler()\n",
    "def transform_coords(data,fit=False):\n",
    "    if fit:\n",
    "        coords_scaler.fit(data)\n",
    "    return coords_scaler.transform(data)\n",
    "\n",
    "category_enc = None\n",
    "def conv_category(data, fit=False):\n",
    "    if fit:\n",
    "        category_enc = preprocessing.LabelEncoder()\n",
    "        category_enc.fit(data)\n",
    "    return category_enc.transform(data)\n",
    "\n",
    "coords_net = None\n",
    "coords_net_size = 50\n",
    "def coords_to_net(coords,fit=False,net_size=50):\n",
    "    if fit:\n",
    "        coords_net = np.array([[np.min(coords[:,0]), np.max(coords[:,0])],\n",
    "                               [np.min(coords[:,1]), np.max(coords[:,1])]])\n",
    "        coords_net_size = net_size\n",
    "    result = np.zeros((coords.shape[0],2))\n",
    "    x_scale = coords_net[0,1] - coords_net[0,0]\n",
    "    y_scale = coords_net[1,1] - coords_net[1,0]\n",
    "    for i in range(0,coords.shape[0]):\n",
    "        x = int(((coords[i,0]-coords_net[0,0])/x_scale)*(net_size-1))\n",
    "        y = int(((coords[i,1]-coords_net[1,0])/y_scale)*(net_size-1))\n",
    "        result[i] = [x,y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((transform_coords(train_data[['X', 'Y']].values,True),\n",
    "                     feature_binarizer(train_data,'Hour',True),\n",
    "                     feature_binarizer(train_data,'Day',True),\n",
    "                     feature_binarizer(train_data,'Month',True),\n",
    "                     feature_binarizer(train_data,'Year',True),\n",
    "                     feature_binarizer(train_data,'DayOfWeek',True),\n",
    "                     #feature_binarizer(train_data,'Address',True),\n",
    "                     append_streets(train_data.Address, streets),\n",
    "                     feature_binarizer(train_data,'PdDistrict',True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = coords_to_net(train_data[['X', 'Y']].values,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train3 = np.hstack((transform_coords(train_data[['X', 'Y']].values,True),\n",
    "                     feature_binarizer(train_data,'Hour',True),\n",
    "                     feature_binarizer(train_data,'Month',True),\n",
    "                     feature_binarizer(train_data,'Year',True),\n",
    "                     feature_binarizer(train_data,'DayOfWeek',True),\n",
    "                     feature_binarizer(train_data,'PdDistrict',True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train4 = train_data[['X', 'Y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = conv_category(train_data.Category.values, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def cross_evaluate_model(clf,X_train,y_train):\n",
    "    start_time = datetime.datetime.now()\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5,scoring='log_loss')\n",
    "    duration = (datetime.datetime.now() - start_time).total_seconds()\n",
    "\n",
    "    print('Classifier: {0}'.format(type(clf)))\n",
    "    print('Time elapsed: {0}'.format(duration))\n",
    "    print('average score (5 folds): {0}'.format(scores.mean()))\n",
    "    \n",
    "def evaluate_model(clf,X,y,test_size=0.8):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    clf.fit(X_train,y_train)\n",
    "    fit_duration = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    y_pred_train = clf.predict_proba(X_train)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    predict_duration = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    train_score = log_loss(y_train,y_pred_train)\n",
    "    test_score = log_loss(y_test,y_pred_test)\n",
    "    \n",
    "    print('Classifier: {0}'.format(type(clf)))\n",
    "    print('Fit time: {0:.1f} sec, Predict time: {1:.1f} sec, All time: {0:.1f} sec'.format(\n",
    "            fit_duration, predict_duration, fit_duration + predict_duration))    \n",
    "    print('Train Score: {0:.2f}'.format(train_score))\n",
    "    print('Test Score: {0:.2f}'.format(test_score))\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        print('Feature importances: {0}'.format(clf.feature_importances_))\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print('Feature importances: {0}'.format(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "Fit time: 109.6 sec, Predict time: 2.9 sec, All time: 109.6 sec\n",
      "Train Score: 2.54\n",
      "Test Score: 2.56\n",
      "Feature importances: [[ 0.4260425  -2.37900431 -0.18095444 ..., -0.55165875 -0.5371095\n",
      "  -0.38077906]\n",
      " [ 0.63097472 -1.32164378 -0.03395671 ..., -0.18995058 -0.27449741\n",
      "   0.1203483 ]\n",
      " [-0.90251563 -1.14900262  1.00811082 ...,  0.07159907 -0.87106772\n",
      "  -0.29468388]\n",
      " ..., \n",
      " [-1.09656555  0.25015721 -0.3082151  ..., -0.66563084  0.08317785\n",
      "  -1.7097407 ]\n",
      " [ 0.32787852 -0.72066001 -0.28781827 ...,  0.15783213 -0.84877703\n",
      "   0.65009328]\n",
      " [ 1.19305218 -1.51775111 -0.14500234 ..., -0.60010389 -0.30176007\n",
      "  -0.16925273]]\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "evaluate_model(LogisticRegression(random_state=241), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "Fit time: 75.7 sec, Predict time: 3.1 sec, All time: 75.7 sec\n",
      "Train Score: 2.55\n",
      "Test Score: 2.56\n",
      "Feature importances: [[ 0.40320172 -2.39265506 -0.19692019 ..., -0.53739474 -0.53571149\n",
      "  -0.39546545]\n",
      " [ 0.59028407 -1.29495601 -0.03281479 ..., -0.18601102 -0.27169606\n",
      "   0.09844458]\n",
      " [-0.91578802 -1.17026874  1.02206048 ...,  0.08327702 -0.87614901\n",
      "  -0.32907348]\n",
      " ..., \n",
      " [-1.0920884   0.24138742 -0.31489525 ..., -0.68077315  0.06549942\n",
      "  -1.6716455 ]\n",
      " [ 0.31065281 -0.7553505  -0.2997602  ...,  0.16615282 -0.86128215\n",
      "   0.68044639]\n",
      " [ 1.20047539 -1.51811054 -0.14228482 ..., -0.61395144 -0.31355415\n",
      "  -0.17782009]]\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "evaluate_model(LogisticRegression(random_state=241), X_train3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "evaluate_model(KNeighborsClassifier(n_neighbors=5, n_jobs=4), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "evaluate_model(RandomForestClassifier(n_estimators=30, max_depth=10, n_jobs=4,\n",
    "                                      verbose=True, random_state=241), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      545703.7535           89.66m\n",
      "         2      504936.3204           80.10m\n",
      "         3      476422.2271           71.12m\n",
      "         4      454011.9280           60.83m\n",
      "         5      435188.5451           50.63m\n",
      "         6      419436.6596           40.40m\n",
      "         7      405828.1451           30.31m\n",
      "         8      393996.1933           20.23m\n",
      "         9      383566.9101           10.10m\n",
      "        10      374426.4463            0.00s\n",
      "Classifier: <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "Fit time: 6058.2 sec, Predict time: 50.4 sec, All time: 6058.2 sec\n",
      "Train Score: 2.13\n",
      "Test Score: 2.68\n",
      "Feature importances: [  1.71809100e-01   1.90983281e-01   6.04780814e-03   3.90130414e-03\n",
      "   5.07280191e-03   3.25905392e-03   2.57038197e-03   2.07971129e-03\n",
      "   3.86272932e-03   4.12840414e-03   5.90287469e-03   5.75341267e-03\n",
      "   6.75773339e-03   7.43860086e-03   6.60888515e-03   8.32867424e-03\n",
      "   7.03182287e-03   5.67695908e-03   7.12835185e-03   5.90558550e-03\n",
      "   7.41895114e-03   5.26412608e-03   4.72151413e-03   5.69360783e-03\n",
      "   4.48455933e-03   5.35053428e-03   5.56094675e-03   5.19648963e-03\n",
      "   4.95388694e-03   4.63249493e-03   4.50827399e-03   4.52595872e-03\n",
      "   5.62469629e-03   5.05766574e-03   4.37798712e-03   4.83703751e-03\n",
      "   5.07581369e-03   5.84911917e-03   4.72597029e-03   4.83330142e-03\n",
      "   5.21841864e-03   5.06157079e-03   4.80093976e-03   5.31204794e-03\n",
      "   4.89453437e-03   4.70300075e-03   3.95593257e-03   4.59533259e-03\n",
      "   4.87766950e-03   4.46109068e-03   5.38071096e-03   4.72394923e-03\n",
      "   5.12576146e-03   5.15078396e-03   5.30894979e-03   6.27719502e-03\n",
      "   2.80292536e-03   9.63910071e-03   9.52966810e-03   8.50486581e-03\n",
      "   8.02712704e-03   9.70086976e-03   7.15245087e-03   7.30689635e-03\n",
      "   8.97356429e-03   8.03551713e-03   9.13388922e-03   7.54914482e-03\n",
      "   1.42386563e-02   1.09390597e-02   8.46872952e-03   9.66485603e-03\n",
      "   6.59743413e-03   1.18318657e-02   9.58832727e-03   1.01240071e-02\n",
      "   1.32657307e-02   9.70367495e-03   1.07231983e-02   8.54642504e-03\n",
      "   8.39898793e-03   4.24741763e-03   1.28916652e-02   9.08863505e-03\n",
      "   7.81009900e-03   8.34003657e-03   1.05494381e-02   1.08958563e-02\n",
      "   1.06356237e-02   9.98607786e-04   3.07199947e-04   5.78909370e-04\n",
      "   4.95607746e-04   6.70643850e-04   1.29786209e-04   3.63387241e-05\n",
      "   4.53060273e-04   1.04526572e-03   2.46857587e-04   8.34850449e-05\n",
      "   5.88397331e-05   1.29794112e-03   5.75294536e-04   2.72822894e-03\n",
      "   2.50359436e-03   3.88890122e-03   6.84599928e-03   4.84374133e-03\n",
      "   3.38422912e-03   2.13655565e-03   7.06607861e-03   2.16899032e-03\n",
      "   1.17618004e-02]\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "evaluate_model(GradientBoostingClassifier(n_estimators=10, max_depth=10,\n",
    "                                          verbose=True, random_state=241), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "evaluate_model(xgb.XGBClassifier(n_estimators=30, max_depth=10, learning_rate=0.2,\n",
    "                                 silent=False, randome_state=241), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Напишем свой классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class MyClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Predicts the majority class of its training data.\"\"\"\n",
    "    def __init__(self, net_size = 50, n_max=5, verbose=False):\n",
    "        self.net_size_ = net_size\n",
    "        self.n_max_ = n_max\n",
    "        self.verbose_ = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = np.max(y) + 1\n",
    "        self.min_x_ = np.min(X[:,0])\n",
    "        self.max_x_ = np.max(X[:,0])\n",
    "        self.min_y_ = np.min(X[:,1])\n",
    "        self.max_y_ = np.max(X[:,1])\n",
    "        self.x_scale_ = self.max_x_ - self.min_x_\n",
    "        self.y_scale_ = self.max_y_ - self.min_y_\n",
    "        net = defaultdict(list)\n",
    "        self.net_weights_ = dict()\n",
    "        for i in range(0,X.shape[0]):\n",
    "            x_ = np.max([np.floor((X[i,0] - self.min_x_)/self.x_scale_ * self.net_size_),self.net_size_-1])\n",
    "            y_ = np.max([np.floor((X[i,1] - self.min_y_)/self.y_scale_ * self.net_size_),self.net_size_-1])\n",
    "            \n",
    "            net[(x_,y_)].append(y[i])\n",
    "        \n",
    "        for k,v in net.items():\n",
    "            cnt = Counter(v).most_common(self.n_max_)\n",
    "            self.net_weights_[k] = np.array([float(cnt[x])/len(v) if x in cnt else 0.0 for x in range(0,self.n_classes_)])\n",
    "            \n",
    "        majority_cnt = Counter(y)\n",
    "        self.majority_ = np.array([float(majority_cnt[x])/len(v) if x in majority_cnt else 0.0\n",
    "                                   for x in range(0,self.n_classes_)])\n",
    "\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        result = np.empty((X.shape[0],self.n_classes_))\n",
    "        \n",
    "        for i in range(0,X.shape[0]):\n",
    "            x_ = np.max([np.floor((X[i,0] - self.min_x_)/self.x_scale_ * self.net_size_),self.net_size_-1])\n",
    "            y_ = np.max([np.floor((X[i,1] - self.min_y_)/self.y_scale_ * self.net_size_),self.net_size_-1])\n",
    "            \n",
    "            if (x_,y_) in self.net_weights_:\n",
    "                result[i] = self.net_weights_[(x_,y_)]\n",
    "            else:\n",
    "                result[i] = self.majority_\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MeanClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Predicts the majority class of its training data.\"\"\"\n",
    "    def __init__(self, classifiers, weights=None):\n",
    "        self.classifiers_ = classifiers\n",
    "        if weights:\n",
    "            self.weights_ = weights\n",
    "        else:\n",
    "            self.weights_ = [1.0/len(classifiers)]*len(classifiers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers_:\n",
    "            clf.fit(X,y)\n",
    "\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        predicts = [clf.predict_proba(X) for clf in self.classifiers_]\n",
    "        \n",
    "        result = np.zeros(predicts[0].shape)\n",
    "        \n",
    "        for i, w in enumerate(self.weights_):\n",
    "            result += w*predicts[i]\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: <class '__main__.MeanClassifier'>\n",
      "Fit time: 35.9 sec, Predict time: 87.0 sec, All time: 35.9 sec\n",
      "Train Score: 3.66\n",
      "Test Score: 3.66\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(MeanClassifier(\n",
    "        [MyClassifier(net_size=100, n_max=15),\n",
    "         MyClassifier(net_size=50, n_max=10),\n",
    "         MyClassifier(net_size=25, n_max=10)]),\n",
    "               X_train4, y_train, test_size=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
